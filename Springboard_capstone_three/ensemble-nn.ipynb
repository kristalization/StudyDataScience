{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport random\nimport os\nimport cv2\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.utils import plot_model\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nimport tensorflow as tf\n\nimport keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nfrom keras import Model, layers\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD\nfrom keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, Input, Conv2D, MaxPooling2D, Flatten,MaxPooling3D\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/intel-image-classification/'\ntrain_pred_test_folders = os.listdir(root_path)\n\nseg_train_folders = os.listdir(os.path.join(root_path,train_pred_test_folders[0],train_pred_test_folders[0])) #one more seg_train folder within\nseg_pred_folders = os.listdir(os.path.join(root_path,train_pred_test_folders[1],train_pred_test_folders[1]))\nseg_test_folders = os.listdir(os.path.join(root_path,train_pred_test_folders[2],train_pred_test_folders[2]))\nquantity_tr = {} \nquantity_te = {}\nfor folder in seg_train_folders:\n    quantity_tr[folder] = len(os.listdir(os.path.join(root_path,train_pred_test_folders[0],train_pred_test_folders[0],folder)))\n\nfor folder in seg_test_folders:\n    quantity_te[folder] = len(os.listdir(os.path.join(root_path,train_pred_test_folders[2],train_pred_test_folders[2],folder)))\n    \nquantity_train = pd.DataFrame(list(quantity_tr.items()), index=range(0,len(quantity_tr)), columns=['class','count'])\nquantity_test = pd.DataFrame(list(quantity_te.items()), index=range(0,len(quantity_te)), columns=['class','count'])\n\nfigure, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x='class',y='count',data=quantity_train,ax=ax[0])\nsns.barplot(x='class',y='count',data=quantity_test,ax=ax[1])\n\nprint(\"Number of images in the train set : \", sum(quantity_tr.values()))\nprint(\"Number of images in the test set ; \",sum(quantity_te.values()))\nnumber_of_images_in_prediction_set = len(os.listdir(os.path.join(root_path,train_pred_test_folders[1],train_pred_test_folders[1])))\nprint(\"Number of images in prediction set : \",number_of_images_in_prediction_set)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_history(history, model_name):\n    #convert the history.history dict to a pandas DataFrame:     \n    hist_df = pd.DataFrame(history.history) \n\n    # save to json:  \n    hist_json_file = model_name+'_history.json' \n    with open(hist_json_file, mode='w') as f:\n        hist_df.to_json(f)\n\n    # or save to csv: \n    hist_csv_file = model_name+'_history.csv'\n    with open(hist_csv_file, mode='w') as f:\n        hist_df.to_csv(f)\n        \ndef plot_accuracy_from_history(history, isinception=False):\n    color = sns.color_palette()\n    if(isinception == False):\n        acc = history.history['acc']\n        val_acc = history.history['val_acc']\n    else:\n        acc = history.history['accuracy']\n        val_acc = history.history['val_accuracy']\n    \n\n    epochs = range(len(acc))\n\n    sns.lineplot(epochs, acc, label='Training Accuracy')\n    sns.lineplot(epochs, val_acc,label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    plt.figure()\n    plt.show()\n    \ndef plot_loss_from_history(history):\n    color = sns.color_palette()\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(len(loss))\n    \n    sns.lineplot(epochs, loss,label='Training Loss')\n    sns.lineplot(epochs, val_loss, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.figure()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_DIR = \"/kaggle/input/intel-image-classification/seg_train/seg_train/\"\ntrain_datagen = ImageDataGenerator( rescale = 1.0/255,shear_range=0.2,zoom_range=0.2)\n\n# we are rescaling by 1.0/255 to normalize the rgb values if they are in range 0-255 the values are too high for good model performance. \ntrain_generator = train_datagen.flow_from_directory(train_DIR,\n                                                    batch_size=32,\n                                                    shuffle=True,\n                                                    class_mode='categorical',\n                                                    target_size=(150, 150))\n\ntest_DIR = \"/kaggle/input/intel-image-classification/seg_test/seg_test/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.0/255) #we are only normalising to make the prediction, the other parameters were used for agumentation and train weights\nvalidation_generator = validation_datagen.flow_from_directory(test_DIR, shuffle=True, batch_size=32, class_mode='categorical', target_size=(150, 150))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_map_classes = {v: k for k, v in validation_generator.class_indices.items()}\nprint(validation_generator.class_indices)\nprint(inv_map_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_few_images(number_of_examples=2, predict_using_model=None):\n    figure1, ax1 = plt.subplots(number_of_examples,len(seg_train_folders), figsize=(20,4*number_of_examples))\n    ax1 = ax1.reshape(-1)\n    axoff_fun = np.vectorize(lambda ax:ax.axis('off'))\n    axoff_fun(ax1)\n    axs = 0\n    for i, folder in enumerate(seg_train_folders):\n        image_ids = os.listdir(os.path.join(root_path,train_pred_test_folders[0],train_pred_test_folders[0],folder))\n        for j in [random.randrange(0, len(image_ids)) for i in range(0,number_of_examples)]:\n            img = image_ids[j]\n            img_path = os.path.join(root_path,train_pred_test_folders[0],train_pred_test_folders[0],folder,img)\n            display = cv2.imread(img_path)\n            plt.axis('off')\n            ax1[axs].imshow(display)\n            title = 'True:'+folder\n            if(predict_using_model):\n                predicted_classname = inv_map_classes[np.argmax(inception_best_model.predict(np.array([display])))]\n                title = title+'\\nPredict :'+predicted_classname\n            ax1[axs].set_title(title)\n            axs=axs+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random architecture\nbenchmark_model = Sequential()\n# Input here is 4D array (batchsize, height, width, channels) - we have already created the train_generator with batch size 32\n# 32 Images of size each 150x150 with 3 color channels will be input into this layer\nbenchmark_model.add(Conv2D(128, kernel_size=7, activation='relu', input_shape=(150,150,3)))\nbenchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\nbenchmark_model.add(Conv2D(64, kernel_size=5, activation='relu'))\nbenchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\nbenchmark_model.add(Flatten())\nbenchmark_model.add(Dense(128,activation='relu'))\nbenchmark_model.add(Dense(6,activation='softmax'))\n\nbenchmark_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nbenchmark_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_model(benchmark_model, to_file='model.png',show_shapes=True, show_layer_names=True)\n# from IPython.display import FileLink\n# FileLink(r'./resnet50_-saved-model-08-acc-0.75.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"bench_mark_-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, min_lr=0.000002)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\nhistory = benchmark_model.fit(train_generator,epochs=20, verbose=1, validation_data = validation_generator,callbacks=[reduce_lr,early_stopping,checkpoint])\n\nsave_history(history, 'benchmark_model')\nbenchmark_model.save(filepath)\nplot_accuracy_from_history(history)\nplot_loss_from_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"vgg16_saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\nvgg16_model = VGG16(pooling='avg', weights='imagenet', include_top=False, input_shape=(150,150,3))\nfor layers in vgg16_model.layers:\n            layers.trainable=False\nlast_output = vgg16_model.layers[-1].output\nvgg_x = Flatten()(last_output)\nvgg_x = Dense(128, activation = 'relu')(vgg_x)\nvgg_x = Dense(6, activation = 'softmax')(vgg_x)\n\nvgg16_final_model = Model(vgg16_model.input, vgg_x)\nvgg16_final_model.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n#plot_model(vgg16_final_model, to_file='model.png',show_shapes=True, show_layer_names=True)\n\n# VGG16\nnumber_of_epochs = 25\nvgg16_filepath = 'vgg_16_'+'-saved-model-{epoch:02d}-acc-{val_acc:.2f}.hdf5'\n\nvgg_checkpoint = tf.keras.callbacks.ModelCheckpoint(vgg16_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nvgg_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n\nvgg16_history = vgg16_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=[vgg_checkpoint,vgg_early_stopping],verbose=1)\n\nsave_history(vgg16_history, 'vgg16_model')\nplot_accuracy_from_history(vgg16_history)\nplot_loss_from_history(vgg16_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nResNet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150,150,3))\n\nfor layers in ResNet50_model.layers:\n    layers.trainable=False\n\nopt = SGD(lr=0.01,momentum=0.7)\nresnet50_x = Conv2D(256, (3, 3), activation='relu')(ResNet50_model.output)\nresnet50_x = MaxPooling2D(pool_size=(3, 3))(resnet50_x)\nresnet50_x = Flatten()(resnet50_x)\nresnet50_x = Dense(128,activation='relu')(resnet50_x)\nresnet50_x = Dense(6,activation='softmax')(resnet50_x)\n\nresnet50_x_final_model = Model(inputs=ResNet50_model.input, outputs=resnet50_x)\nresnet50_x_final_model.compile(loss = 'categorical_crossentropy', optimizer= opt, metrics=['acc'])\n#plot_model(ResNet50_model, to_file='model.png',show_shapes=True, show_layer_names=True)\nnumber_of_epochs = 20\nresnet_filepath = 'resnet50'+'-saved-model-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5'\n\nresnet_checkpoint = tf.keras.callbacks.ModelCheckpoint(resnet_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nresnet_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n\nresnet50_history = resnet50_x_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=[resnet_checkpoint,resnet_early_stopping],verbose=1)\n\nsave_history(resnet50_history, 'resnet50_model')\nplot_accuracy_from_history(resnet50_history)\nplot_loss_from_history(resnet50_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this could also be the output a different Keras model or layer\n\nInceptionV3_model = InceptionV3(input_shape=(150,150,3),weights='imagenet', include_top=False)\nfor layer in InceptionV3_model.layers[:249]:\n   layer.trainable = False\nfor layer in InceptionV3_model.layers[249:]:\n   layer.trainable = True\nInceptionV3_last_output = InceptionV3_model.output\nInceptionV3_maxpooled_output = Flatten()(InceptionV3_last_output)\nInceptionV3_x = Dense(2056, activation='relu')(InceptionV3_maxpooled_output)\nInceptionV3_x = Dropout(0.5)(InceptionV3_x)\nInceptionV3_x = Dense(1024, activation='relu')(InceptionV3_maxpooled_output)\n# and a logistic layer -- let's say we have 200 classes\nInceptionV3_x = Dense(6, activation='softmax')(InceptionV3_x)\nInceptionV3_x_final_model = Model(inputs=InceptionV3_model.input,outputs=InceptionV3_x)\nInceptionV3_x_final_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=['accuracy'])\n#plot_model(InceptionV3_x_final_model, to_file='model.png',show_shapes=True, show_layer_names=True)\n\nnumber_of_epochs = 20\ninception_filepath = 'inceptionv3_'+'-saved-model-{epoch:02d}-loss-{loss:.2f}.hdf5'\n\ninception_checkpoint = tf.keras.callbacks.ModelCheckpoint(inception_filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n\ninception_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n\n#inception_best_model = keras.models.load_model('../input/pretrained-models-on-intel-image-classification/inception_loss_0.32_val_loss_0.64')\ninceptionv3_history = InceptionV3_x_final_model.fit(train_generator, epochs = number_of_epochs, validation_data = validation_generator,callbacks=[inception_checkpoint,inception_early_stopping],verbose=1)\n\nsave_history(inceptionv3_history, 'inceptionv3_model')\n# we don't have accuracy information for inceptionv3\nplot_accuracy_from_history(inceptionv3_history,True)\nplot_loss_from_history(inceptionv3_history)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining the best models of VGG16, Resnet50 & InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# vgg_best_model = keras.models.load_model('../input/pretrained-models-on-intel-image-classification/vgg_16_-saved-model-25-acc-0.89.hdf5')\n# resnet_best_model = keras.models.load_model('../input/pretrained-models-on-intel-image-classification/resnet50_-saved-model-40-acc-0.82.hdf5')\n# inception_best_model = keras.models.load_model('../input/pretrained-models-on-intel-image-classification/inceptionv3_-saved-model-03-loss-0.22.hdf5')\n\nvgg_best_model = vgg16_final_model \nresnet_best_model = resnet50_x_final_model\ninception_best_model = InceptionV3_x_final_model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef mode(my_list):\n    ct = Counter(my_list)\n    max_value = max(ct.values())\n    return ([key for key, value in ct.items() if value == max_value])\n\ntrue_value = []\ncombined_model_pred = []\nvgg_pred = []\nresnet_pred = []\ninception_pred = []\nbenchmark_model_pred = []\nfor folder in seg_test_folders:\n    \n    test_image_ids = os.listdir(os.path.join(root_path,train_pred_test_folders[2],train_pred_test_folders[2],folder))\n    \n    for image_id in test_image_ids[:int(len(test_image_ids)/4)]:\n        \n        path = os.path.join(root_path,train_pred_test_folders[2],train_pred_test_folders[2],folder,image_id)\n        \n        true_value.append(validation_generator.class_indices[folder])\n        img = cv2.resize(cv2.imread(path),(150,150))\n        \n        #vgg\n        vgg16_image_prediction = np.argmax(vgg_best_model.predict(np.array([img])))\n        vgg_pred.append(vgg16_image_prediction)\n        \n        #resnet50\n        resnet_50_image_prediction = np.argmax(resnet_best_model.predict(np.array([img])))\n        resnet_pred.append(resnet_50_image_prediction)\n        \n        #Inception\n        inception_image_prediction = np.argmax(inception_best_model.predict(np.array([img])))\n        inception_pred.append(inception_image_prediction)\n        \n        #benchmark\n        benchmark_model_prediction = np.argmax(benchmark_model.predict(np.array([img])))\n        benchmark_model_pred.append(benchmark_model_prediction)\n        \n        #giving vgg16 high priority if they all predict something different\n        image_prediction = mode([vgg16_image_prediction, resnet_50_image_prediction, inception_image_prediction])                                  \n        combined_model_pred.append(image_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n#from mlxtend.plotting import plot_confusion_matrix\ndef clf_report(true_value, model_pred):\n    \n    classes = validation_generator.class_indices.keys()\n    TP_count = [true_value[i] == model_pred[i] for i in range(len(true_value))]\n    model_accuracy = np.sum(TP_count)/len(TP_count)\n    print('Model Accuracy', model_accuracy)\n    \n    plt.figure(figsize=(7,7))\n    cm = confusion_matrix(true_value,model_pred)\n    plt.imshow(cm,interpolation='nearest',cmap=plt.cm.viridis)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max()*0.8\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,cm[i,j],\n                horizontalalignment=\"center\",\n                color=\"black\" if cm[i,j] > thresh else \"white\")\n        pass\n    \n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    pass\n    \n    print(classification_report(true_value, model_pred, target_names = list(classes)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_report(true_value, benchmark_model_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combined vote\ncombined_model_pred = [ c[0] for c in combined_model_pred]\nclf_report(true_value, combined_model_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VGG model classification report\nclf_report(true_value, vgg_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resnet50 model classification report\nclf_report(true_value, resnet_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inception model classification report\nclf_report(true_value, inception_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(1,benchmark_model_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(1,vgg_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(1,resnet_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(1,inception_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_few_images(1,combined_model_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}